# 回归

回归(Regression)是机器学习中重要的解题方法。

回归分析，简单来说就是“由果索因”的过程，是一种归纳思想 —— 当看到大量事实所呈现的样态，推断其原因；当看到大量数字对是某种样态，推断它们之间的蕴含关系。

## 线性回归

线性回归是利用数理统计学中的回归分析来确定两种或两种以上变量之间相互依赖的定量关系的一种统计分析方法。其表达形式如下：

$y = ax+b+e$

$e$ 为误差服从均值为 0 的正态分布。

统计统计或者实验，可能会得到两种值的对应关系，这两种值一种是 $y$，一种是 $x$，每组 $y$ 和 $x$ 是成对出现且一一对应的，最后可以用一种 $y=ax+b+e$ 的表达式来表示它们的关系。

其中 $e$ 的值满足正态分布， $\mu$ 为 0。

## 拟合

回忆“打点计时器测试重力加速度”的实验

通过打点的位移差，结合公式，可以计算出 n 个时间点的的瞬时速度，在坐标系中将这些散点用一条直线连接，就得到一条粗糙的直线。

这种从大量函数结果和自变量反推函数表达式的过程就是 **回归**。

而将平面上的散点用一条光滑的曲线连接起来的过程就叫做 **拟合**。

[使用 python 线性回归拟合重力加速度实验曲线](./code/08.线性回归拟合重力加速的.py)

## 残差分析

前面提到，在线性回归中希望最终得到的是一个 $y=ax+b+e$ 的函数来表示 $y$ 和 $x$ 的关系。

但是，从打点计时器测试重力加速度的实验来看，理论上 $v=gt$，而在实验中得出的是一个不太准确的 $v=gt+e$，（这里没有 b 是因为初速度 $v_0$ = 0）。

可以明显的看出， $e$ 就是误差值。

那么，$g$ 取多少，才能让 $e$ 最小呢？

这个过程，就是 **残差分析**。

最终的目标就是，计算出一个 $g$，使得 $e$ 为误差服从均值为 0 的正态分布。定性的来说，就是在拟合过程中，每个点产生的误差 $e$ 大部分都在 0 附近，远离 0 的越少越好。

在线性回归中，有一种非常经典的系数猜测方法 —— 最小二乘法。

### 最小二乘法的简单原理

假设有多个 $x$ 和 $y$ 的样本值，同时尝试用 $y=ax+b+e$ 来拟合，可以得到：

$$|e| = |ax+b-y|$$

试着将 $|e|$ 求和（因为是绝对值求和，可以转变为平方后求和），构造一个函数：

$$Q=\sum_{i=i}^n(ax_i+b-y_i)^2$$

$Q$ 指根据每一组样本里的 $x$ 拟合得到的 $y$ （也就是 $ax+b$）和观察到样本里的 $y$ 都做一个差，将差平方后求和。 $Q$ 就是 $|e|^2$ 的和。

现在问题就转换为当 $a$ 和 $b$ 为多少时 $Q$ 的值最小。

即分别使

$$\frac{\partial Q}{\partial a}=0$$
$$\frac{\partial Q}{\partial b}=0$$

$Q$ 分别对 $a$ 和 $b$ 求偏微分，满足每个偏微分方程为 0 的 $a$、$b$ 变量的值就是要找的值。

$\frac{\partial Q}{\partial a}=0$ 表示函数 $Q$ 对自变量 $a$ 求导，它的结果又是一个函数，而这个新的函数称之为 **导数**。

一般来说，导数里面还是会由 $a$ 作为自变量的，而导数的函数值就是 $Q$ 这个函数在 $a$ 点上的切线斜率。

可以想像，$Q$ 是一个曲线函数，既然时曲线，在每个点上就有切线，让切线随着 $Q$ 函数的曲线滑动，切线的斜率也会跟着变化，而 $Q$ 函数的极致应该在切线斜率为 0 外。

解出上面的两个导数方程：

$$\frac{\partial Q}{\partial a} = {2\sum_{i=1}^n[x_i(ax_i+b-yi)] = 0}$$
$$\frac{\partial Q}{\partial b} = {2\sum_{i=1}^n(ax_i + b - y_i) = 0}$$

将上式展开，$a$ 和 $b$ 作为系数提出去之后，得到：

$${a\sum_{i=i}^n(x_i^2)} + b{\sum_{i=1}^n(x_i)}={\sum_{i=1}^ny_ix_i}$$

同理：

$${a\sum_{i=i}^n(x_i)} + b{\sum_{i=1}^n}={\sum_{i=1}^ny_i}$$

这样就得到一个方程组：

$$
{\begin{cases}
{a\sum_{i=i}^n(x_i^2)} + b{\sum_{i=1}^n(x_i)}={\sum_{i=1}^ny_ix_i} &&& (1)\\
{a\sum_{i=i}^n(x_i)} + Nb = {\sum_{i=1}^ny_i}&&& (2)
\end{cases}}
$$

将 $(2) \cdot { {\sum_{i=1}^nx_i\over N} - (1)}$

得到下面等式：

$${a({\sum_{i=1}^nx_i}\cdot{{\sum_{i=1}^nx_i}\over{n}}-{\sum_{i=1}^nx_i^2})= {{\sum_{i=1}^ny_i}\cdot{{\sum_{i=1}^nx_i}\over{n}}-{\sum_{i=1}^ny_ix_i}}}$$

可以得到：

$${a}={{{{{\sum_{i=1}^ny_i}\cdot{\sum_{i=1}^nx_i}}\over{n}}-{\sum_{i=1}^ny_ix_i}}\over{{{{\sum_{i=1}^nx_i}\cdot{\sum_{i=1}^nx_i}}\over{n}}-{\sum_{i=1}^nx_i^2}}}$$

$${b}={{{\sum_{i=1}^ny_i}-{a\sum_{i=1}^nx_i}}\over{n}}$$

其中 $a$ 就是斜率， $b$ 就是截距

将所有的观测值 $x$、$y$ 都代入，就可以得到 $a$；再将观测值与 $a$ 代入，得到 $b$；

[Python 使用最小二乘法进行线性拟合](./code/08.线性回归拟合重力加速度_最小二乘法.py)

## 过拟合

过拟合，简称过拟，是在拟合过程中出现的一种做过头的表现。

拟合，就是通过对数据样本的观察和抽象，最后归纳得到的一个完整的数据映射模型。

但是，在归纳的过程中，可能为了迎合所有样本向量点甚至噪声点而得到的模型过复杂，这就造成过度拟合。

过拟合的危害：

- 描述复杂。所有的过度拟合模型都有一个共同点，那就是模型描述非常复杂 —— 参数繁多，计算逻辑多。
- 失去泛化能力。所谓泛化能力就是通过学习得到的模型对未知数据的预测能力，即应用于其他非训练样本的向量时的分类能力。对于待分类样本向量分类正确度高，表示泛化能力比较好；反之，如果对于待分类样本向量分类正确度低，则表示泛化能力较差。

过拟合的导致原因：

- 训练样本太少。训练样本过少的情况，通常会归纳出一个非常不准确的模型。
- 过于追求完美。训练样本向量点都希望用拟合模型覆盖。但实际上的训练样本却是带有很多噪声点的。比如测试两地之间的行车距离，因车辆故障导致的测试时间极大，或者因计时器故障导致测试时间极小，这些都属于噪声点。

## 欠拟合

欠拟合，简称欠拟，与过拟相反，是由于操作不当或者建模不当导致产生的误差 $e$ 分布太散或者太大的情况。

欠拟合的导致原因：

- 参数过少。对于训练样本向量的维度提取太少会导致模型描述的不准确。

  例如，判断银行储户的信誉度，通常综合考虑用户的余额、年龄、流水和、借贷频率、借贷额度、还贷准时程度等，维度考虑的越充分，拟合程度越高；反之，如果只考虑余额一个维度，强行建模，这种模型就是不科学的。

- 拟合不当。拟合不当的原因比较复杂，通常是拟合方法不正确导致的。

  例如，某个训练样本向量 $x$ 与 结果 $y$ 之间的关系如下：

  $$
  (1, 1)\\
  (1.41, 2)\\
  (1.7, 3)\\
  (2.1, 4)\\
  $$

  这四组数据，用 $y=x^2$ 在第一象限的曲线拟合的误差更小。而如果用 $y=3.732x -2.932$ 的直线做拟合，显然误差要大一些。

## 曲线拟合转换为线性拟合

非线性回归的情况太过复杂，在生产实践中也尽量避免使用这种模型。

非线性回归一般可以分为一元非线性回归和多元非线性回归。

**一元非线性回归**是指两个变量 —— 一个自变量，一个因变量之间呈现非线性关系，如：双曲线、二次曲线、多次曲线、幂曲线、指数曲线、对数曲线等，在解决这些问题时通常建立非线性回归方程或者方程组。

**二元非线性回归**是指两个或两个以上自变量和因变量之间呈现的非线性关系建立非线性回归模型。对多元非线性回归模型求解的传统做法，仍然是将其转换为标准的线性形式的多元回归模型来处理。有些非线性回归模型，经过适当的数学变换，便能得到它的线性化表达式，但对另外一些非线性回归模型，仅仅做变量变化根本无济于事。属于前一种的情况的非线性回归模型一般称为内蕴的线性回归，而后者称之为内蕴的非线性回归。

线性拟合最简单的就是 $y=ax+b$ 这种形式。除此之外，稍微复杂点的也可以将等式两边转换为几个一次式相加的情况，无论是一元还是多元，都仍然是线性回归研究的范畴。

例如：20 世纪 60 年代世界人口状况

| 年份 | 世界人口(亿) | 年份 | 世界人口(亿) |
| :--: | :----------: | :--: | :----------: |
| 1960 |    29.72     | 1965 |    32.85     |
| 1961 |    30.61     | 1966 |    33.56     |
| 1962 |    31.51     | 1967 |    34.20     |
| 1963 |    32.13     | 1968 |    34.83     |
| 1964 |    32.34     |

根据马尔萨斯人口模型：

$$s = \alpha \cdot e^{\beta t}$$

其中 $s$ 是人口， $t$ 是年份， $e$ 是自然常数（约 2.71828），尝试推导 2030 年世界人口数量。

这个问题是一个比较典型的多元线性回归的问题模型。

求解如下：

对等式两边同时取 $ln$， 得到：

$$ln s = \beta t + ln \alpha$$

这里实际上用到的还是线性回归模型，相当于：

$$y=ax+b$$

且

$$
ln s = y\\
\beta = a\\
ln \alpha = b
$$

[Python 拟合世界人口数量模型](./code/08.线性回归拟合世界人口数量模型.py)

横轴上的点是时间 1960~1968，纵轴是 $lnS$ 的值，斜率是 $\beta$ 的值，截距是 $ln \alpha$ 的值，通过计算，可以得到：

$\beta \approx 0.01859$

$\alpha \approx 4.48401395866716 \times 10^{-15}$

带入公式 $s = \alpha \cdot e^{\beta t}$

得到 2000 年时，预测 $s$ 约为 62.9 亿

---

需要注意的是：回归模型只能用来预测和自变量统计区间比较近的自变量对应的函数值。

例如：预测 2000 年的人口数据，误差在 3.1%，相对比较精确，但是如果预测 4000 年，就会得到 88263 亿，这个数据明显已经失真。
