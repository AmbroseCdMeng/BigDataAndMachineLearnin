# 聚类

聚类指的是一种学习方式（操作方式），即把物理或抽象对象的集合分组为由彼此类似的对象组成的多个类的分析过程。

人类天生就有这种能够把类似的事物放到一起作为一类事物进行认识的能力，它们之间可能有彼此的不同，但是有一个限度，只要在这个限度内，特征稍有区别无关大碍，它们仍是一类事物。

在没有人特意教授不同小种群的称谓和特征之前，人类自然具备这种主观认知能力，以特征形态相同或近似将它们划在一个概念下，以特征形态的不同划在不同的概念下，这本身就是聚类的思维方式。

## K-Means 算法

在聚类中 K-Means 算法是很常用的算法，也是基于向量距离来做聚类。

主要步骤如下：

1. 从 $n$ 个向量对象任意选择 $k$ 个向量作为初始聚类中心；
2. 根据 (1) 中选择的 $k$ 个向量（中心对象向量），计算每个对象与这 $k$ 个中心对象各自的距离；
3. 对于 (2) 中的计算，任何一个向量与这 $k$ 个向量都有一个距离，有的远有的近，把这个向量和距离它最近的中心向量对象归在一个类簇中。
4. 重新计算每个类簇的中心对象向量位置；
5. 重复 (3)(4) 两个步骤，直到类簇聚类方案中的向量归类变化极少为止。例如，某一次迭代后，只有少于 1% 的向量还在发生类簇之间的归类漂移，那么就可以认为分类完成。

例如：

[K-Means 算法聚类分析中国城市经纬度](./code/09.KMeans算法聚类分析中国城市经纬度.py)

## 孤立点

在聚类的过程中，会常常碰到一些离主群或者离每个群都非常远的点，这种点就叫做 **孤立点**，也叫离群点。

孤立点在很多数据研究材料中时专门作为一类研究方法来研究的。

聚类算法通常不能直接解释孤立点产生的原因，但是孤立点的存在也是有具体意义的。

第一：孤立点可能是由于数据清洗不当而产生的，这属于操作性的失误问题，不属于研究范围。，

第二：孤立点通常由一些和群里的个体特点差异极大的样本组成。

## 层次聚类

K-Means 算法是直接将样本分成若干个群；

层次聚类就是通过聚类算法把样本根据距离分成若干个大群，大群之间相异，大群内部相似，而大群内部又当成一个全局的样本空间，在继续划分成若干小群，小群之间差异，小群内部相似。

这就是层次聚类思想。最后形成一棵树的结构。

层次聚类从思考角度来说，有两种思路。一种是“凝聚的层次聚类方法”，一种是“分裂的层次聚类方法”。

**凝聚的层次聚类方法**就是在大量的样本中自底而上找那些距离比较近的样本先聚合成小群，聚合到一定程度后由小群聚合成大群。

**分裂的层次聚类方法**就是先把所有样本分成若干大群，再在每个大群中各自重新进行聚类划分。

以凝聚层次聚类方法为例，其工作思路如下：

1. 将整个待分类样本看做一棵完整的树，树根是所有的训练样本向量，而众多树叶就是每一个单独的样本。
2. 设计观察点，让它们散布在整个训练样本中。让这些观察点自上而下不断地进行类簇的合并。

这种聚类合并也是遵循一定原则的，即基于连接度的度量来判断是否要向上继续合并两个类簇。

度量有以下 3 种不同的策略原则：

- Ward 策略。让所有类簇中方差最小化。
- Maximum 策略。也叫 Completed linkage（全连接策略），力求将类簇之间的距离最大值最小化；
- Average linkage 策略。力求将簇之间的距离的平均值最小化。

[Ward 策略的凝聚的层次聚类实例](./code/09.凝聚的层次聚类算法_Ward策略.py)

## 密度聚类

密度聚类很多时候用在聚类形状不规则的情形下。

K-Means 算法是用欧氏距离半径来进行类簇划分的，对于类似拐弯、狭长、不规则形状的聚类效果没有圆形类簇的效果好。

需要注意的是，K-Means 的距离计算公式也是可以选取的，一般用欧式距离比较简单，或者用曼哈顿距离。

常用的距离度量方法包括欧式距离和余弦相似度。二者都是评定个体间差异大小的。

欧式距离度量会受指标不同单位刻度的影响，所以一般需要先进行标准化或归一化，同时距离越大，个体间差异越大；

空间向量余弦夹角的相似度度量不会受指标刻度的影响，余弦值落于区间 [-1, 1] 上，值越大，差异越小。

[DBSCAN 密度聚类分析国土面积实例](./code/09.DBSCAN密度聚类分析国土面积.py)

## 聚类评估

聚类算法有很多种，但是，聚类的好坏如何判断？

聚类的质量评估包括以下 3 个方面：

1. 估计聚类的趋势。对于给定的数据集，评估该数据集是否存在非随机结构，也就是分布不均匀的情况。如果直接使用各种算法套用在样本上，然后返回一些类簇，这些类簇的分类很可能是一些错误的分类，会对人们产生误导。数据中必须存在非随机结构，聚类分析才是有意义的。
2. 确定数据集中的簇数。K-Means 算法在已开始就确定了类簇的数量，并作为参数传递给算法。
3. 测量聚类的质量。可以用量化的方法来测量聚类的质量

## 聚类趋势

如果样本空间的样本是随机出现的，本身没有聚类的趋势，那么使用聚类肯定有问题的。

聚类样本分布样态非常不同时，常用 **霍普金斯统计量** 来进行量化评估。

霍普金斯统计步骤：

1. 从所有样本向量中随机找 $n$ 个向量，把它们称为 $p$ 向量，每一个向量分别是 $p_1、p_2、...、p_n$ 。对每一个向量都在样本空间中找到一个离其最近的向量，然后求距离（用欧氏距离），然后用 $x_1、x_2、...、x_n$ 表示这个距离。
2. 在所有样本向量中随机找 $n$ 个向量，把它们称为 $q$ 向量，每一个向量分别是 $q_1、q_2、...、q_n$ 。对每一个向量都在样本空间里找一个离其最近的向量，然后求距离（用欧氏距离），然后用 $y_1、y_2、...、y_n$ 表示这个距离。
3. 求霍普金斯统计量 $H$ ：

$$H={\sum_{i=1}^{n}{y_i}}\over{{\sum_{i=1}^n{x_i}}+{\sum_{i=1}^n{y_i}}}$$

上式中，分子就是把所有第二次找出的 $q$ 向量的每个向量的临近向量求距离然后加和。分母是两项的加和，一项是分子，一项是 $p$ 向量的每个向量的临近向量的距离加和。

如果整个样本空间是一个分布均匀的，没有聚类趋势（聚簇不明显）的空间，那么 $H$ 应该为 0.5 左右，反之，如果是有聚类趋势（聚簇明显）的空间，那么 $H$ 应该接近于 1。

霍普斯特统计量不是一个常用的统计指标，相关资料较少。

[霍普斯特统计量聚类分析国土面积实例](./code/09.霍普斯特统计量聚类分析国土面积实例.py)

## 簇数确定

确定一个样本空间里有多少簇数是很重要的，尤其是 K-Means 这种算法一开始就要给定被分成的簇数。

而且，簇数的猜测会影响到聚类的结果，簇数太多，样本被分成很多小簇，簇数太少，样本基本没有分开而失去了意义。

**肘方法（The Elbow Method）是一种较为科学的计算簇数的方法**

思路如下：

尝试将样本空间划分为 $n$ 个类，要确定哪种分法更科学，在分成 $m$ 个类簇的时候有一个划分方法，在这种划分方法下，每个类簇的内部都有若干个向量，计算这些向量的空间中心点，即计算这 $m$ 个类簇各自的空间重心在哪里。在计算每个类簇中每个向量和该类簇重心的距离的和。最后把 $m$ 个类簇各自的距离相加得到一个函数 $var(n)$， $n$ 就是类簇数。

[K-Means 算法计算肘方法函数值实例](./code/09.KMeans算法计算肘方法函数值.py)

## 测定聚类质量

聚类的方法有很多，即便类簇数量一样，聚类也有很多种方案。

测定聚类质量的方法一般分为“外在方法”和“内在方法”两种。

所谓外在方法就是一种依靠类别基准的方法，即已经有比较严格的类别定义时再讨论聚类是不是足够准确。通常使用“**BCubed 精度**”和“**BCubed 召回率**”来进行衡量。但是外在方法适用于有明确的外在类别基准的情况，而聚类是一种无监督学习，更多的情况是在不知道基准的状况下进行的，所以我们更倾向于使用“内在方法”。

内在方法不会去参考类簇的标准，而是使用轮廓系数进行度量。

对于有 $n$ 个向量的样本空间，假设分为 $k$ 个类簇，即 $C_1、C_2、...、C_k$。对于任何一个样本空间中的向量 $v$ 来说，可以求一个 $v$ 到本类簇中其它各点的距离的平均值 $a(v)$，还可以求一个 $v$ 到其它所有类簇的最小平均距离（即从每个类簇里挑选一个离 $v$ 最近的向量，然后计算距离），求这些距离的平均值，得到 $b(v)$，轮廓系数的定义为：

$$s(v) = {{b(v) - a(v)}\over{max[a(v), b(v)]}}$$

一般来说，这个函数结果在 -1 ~ 1 之间。$a(v)$ 表示类簇内部的紧凑型，越小越紧凑，而 $b(v)$ 表示该类簇和其它类簇之间的分离程度。如果函数值接近 1，即 $a(v)$ 比较小而 $b(v)$ 比较大时，说明包含 $v$ 的类簇非常紧凑，而且远离其它的类簇。相反，如果函数值为负数，则说明 $a(v) > b(v)$，$v$ 距离其它类簇比距离自己所在的类簇的其它对象近，那么这种情况质量就不太好，应该尽量避免。

[K-Means 算法分类后做质量评估实例](./code/09.KMeans算法分类后做质量评估.py)

在这类方案下，得到的轮廓系数为 0.727，如果找不到更好的方案，这个就已经是最优解了。

也可以修改簇数，查看轮廓系数，探索是否存在更优解。
