# 信息论

信息论无疑是 20 世纪最伟大的发明之一。

信息论的奠定为后来的通信系统、数据传输、密码学、数据压缩等学科领域带来了更多的提示和理论依据，也极大的促进了这些学科领域的长足发展。

## 信息的定义

**信息** 就是被消除的不确定性。

举个例子：

一场二人比赛，结果有可能是 A 胜、B 胜、平局三种情况。这个比赛结果就具有不确定性。

然而当比赛结果确定时，比如 A 胜了，那么 B 胜和平局两种情况就不可能存在了，所以说，此时，这场比赛的结果就确定了。

而用来消除这个不确定性的某句话或者某件事，就是信息。

如果 C 已经知道了比赛结果是 A 胜，那么你在告诉他 B 败了，就是没有意义的，因为此时此事对 C 来说已经确定了，也就是说这句话并没有消除不确定性，所以这句话对 C 来说并不算信息；但是如果 D 并没有看比赛，不知道比赛结果，那么这句话对 D 来说就是消除了不确定性，所以这句话对 D 来说就是信息。

## 信息量

在信息论中，对信息量是有确定解释并且可以量化计算的。

若信源有 m 种消息，且每个消息是以相等可能产生的，则该信源的信息量可表示如下：

$$I(m)=log_2 m$$

该公式实际是一个以 m 为自变量的对数函数

例如，一场二人比赛只存在胜负，不存在平局，则这场比赛只有 A 胜且 B 败、B 胜且 A 败这两种可能，也就是说信源有 2 中信息，那么其信息量就为：

$$I(m)=log_2 2=1$$

单位是比特(bit)

以上公式是建立在一个重要前提下的，那就是 **每个消息是相等可能产生**

而实际情况中，像比赛，明显存在这水平技术等影响因素，比如乒乓球比赛，中国队胜的可能肯定要高于其他国家，那么中国队胜与中国队负这两个信息就不是等可能的，这种情况下，信息量的计算就需要加入一个参考值——先验概率。

所谓 **先验概率** ，就是这件事情按照常理和一般规律发生的概率。例如 A 和 B 在此之前交手 64 次，胜 63 负 1。那么 63/64 就是 A 胜这个信源的先验概率。

用统计学的属于来讲，事件出现的概率越小，事件的信息量就越大，即信息量的多少与事件发生的频繁程度（即概率大小）恰好相反的。切记，相反并不一定成反比。

所以，这种情况下，信息量的计算公式如下：

$$H(X_i)=-log_2 P$$

$X_i$ 表示一个发生的事件，$P$ 表示这个事件发生的先验概率。

用上面的例子来计算， A 胜的先验概率为 63/64

那么，“A 胜” 这个信源的信息量为：

$$H(X_i)=-log_2 {63\over64}\approx0.023 (bit)$$

“B 胜” 这个信源的信息量为：

$$H(X_i)=-log_2 {1\over64}\approx6 (bit)$$

## 香农公式

香农公式是一个非常经典的信息论公式，主要应用在通信工程方面。

无线路由的通信传输速度就是使用香农公式来进行计算的。

$$C=B\cdot{log_2 {(1+{S\over N})}}$$

单位 bps

其中：

- $B$是码元速率的极限值（$B=2H$。 $H$ 为通信带宽，单位为 Baud）；
- $S$是信号功率（瓦）；
- $N$是噪声功率（瓦）。

公式左边的 $C$ 表示一个信道里面的信号传输速度上限，单位 bit。

公式右边的 $B$ 是和带宽大小 $H$ 成正比的，带宽越大传输速度越快。

$S\over N$ 是信噪比，就是要传输的信号功率和这个信道里面产生的信号噪声的功率大小比值。

## 熵

**熵**这个词在热力学、生物学、信息学上都出现过。

### 热力熵

关于热力熵的概念，用一个简单的例子来描述：

在 U 型槽中，放一个铁球，没有空气阻力和摩擦力的情况下，小球会由于能量守恒在 U 型槽中反复运动。但这只是理想状态。

实际状况中，空气阻力以及摩擦力使一部分能量从重力势能向动能转换的过程中损耗掉，整个过程中，越来越多的机械能不可逆的转换为了内能，温度会升高，从而就产生了熵的增加。

### 信息熵

信息熵可以认为是信息的杂乱程度的量化描述。

公式如下：

\$\$H(x)=-\sum\\_{i=1}^n{p(x_i)log_2{P(x_i)}}\\$， \\$i=1,2,...,n\\$\$

其中，$x$ 可以当做一个向量，就是若干个 $x_i$ 产生的概率乘以该可能性的信息量，然后各项做加和运算。

在实际应用过程中， $log$ 取 10 为底数 $lg$ 或者自然常数 $e$ 为底数 $ln$ 都是可以的。但是参与本次应用的所有信息都必须采用同一个底数，否则将没有任何意义。

举例演示：

1. 示例一

   只有胜负的二人比赛中，A 与 B 交手 64 次，A 胜 63 场，B 胜 1 场

   “A 获胜” 的信息量

   $$H(X_i)=-log_2{63\over 64}\approx0.023$$

   “B 获胜” 的信息来那个

   $$H(X_i)=-log_2{1\over 64}\approx6$$

   所以，这场比赛结果的信息熵为：

   $$H(X_i)=0.023 \times {63\over 64} + 6 \times {1\over64}\approx0.1164$$

   根据这个计算公式，可以看出，这是一个二选一的情况，并且确定性相当高的事件的熵。

2. 示例 2

   只有胜负的二人比赛中，A 与 B 交手 64 次，比分为 32:32

   “A 获胜” 的信息量

   $$H(X_i)=-log_2{32\over 64}=1$$

   “B 获胜” 的信息来那个

   $$H(X_i)=-log_2{32\over 64}=1$$

   所以，这场比赛结果的信息熵为：

   $$H(X_i)=1 \times {32\over 64} + 1 \times {32\over64}=1$$

   根据这个计算公式，可以看出，这是一个二选一的情况，并且不确定性相当高（即等概率）的事件的熵。

同样，可以再试一试多选一&等概率 和 多选一&一边倒 两种情况的信息熵。

最终可以得到如下结论：

- 在信息可能有 N 种情况时，如果等概率，那么 N 越大，信息熵越大；
- 在信息可能有 N 种情况时，当 N 一定时，那么其中所有情况概率相等时信息熵是最大的，而如果有一种情况的概率比其他概率都大得多，那么信息熵就越小；

也就是说：

- 信息越确定，越单一，信息熵越小；
- 信息越不确定，越混乱，信息熵越大；

**信息熵**在后期的树算法和文本挖掘以及机器学习中，经常被用来进行条件的优化或者断定一个句子断句方案和各自成句的可能性。
